{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import requests, json, logging, time, urllib\nimport pandas as pd\nfrom io import StringIO\nimport io as stringIOModule\nfrom datetime import datetime\nfrom optparse import OptionParser\nimport configparser\n\n"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Set up logging to catch errors, etc...\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nhandler = logging.FileHandler(str(time.strftime(\"%d_%m_%Y\")) +\"_looker_API_Calls\" + \".log\")\nhandler.setLevel(logging.INFO)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nlogger.info('Testing Logs')"
        }, 
        {
            "source": "# Connect to Looker\nLookerAPI class contains information on connecting to Looker via the API. \n\nThe calls available via this api class allow you to \n1. Establish a connection to the API \n2. Get Look Results (run_look)\n3. Get Query Results based on Query Slug (run_query_slug)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "class LookerAPI(object):\n    \"\"\"Class to contain methods and variables related to looker API authentication and Requests\n    \"\"\"\n    def __init__(self, api_endpoint, client_id, client_secret, login_url):\n        self.api_endpoint = api_endpoint\n        self.client_secret = client_secret\n        self.client_id = client_id\n        self.login_endpoint = login_url\n#         print(self.login_endpoint)\n        \n    def login(self):\n        \"\"\"login to looker API\"\"\"\n        try:\n            auth_data = {'client_id':self.client_id, 'client_secret':self.client_secret}\n            r = requests.post( self.login_endpoint,data=auth_data) # error handle here\n            json_auth = json.loads(r.text)['access_token']\n            return json_auth\n        except requests.exceptions.RequestException as e:\n            logger.error(e)\n\n    def run_look(self, look_id, json_auth,return_format='csv'):\n        \"\"\"run look and return as csv, need to add more formats here\"\"\"\n        try:\n            look_run_url = self.api_endpoint + '/looks/{0}/run/{1}'.format(look_id,return_format)\n            #r = requests.get(look_run_url, headers={'Authorization': \"token \" + json_auth})\n            r = requests.get(look_run_url + '?' + 'access_token=' + json_auth)\n            return r.text\n        except requests.exceptions.RequestException as e:\n            logger.error(e)\n            \n    def run_query(self, query_id, json_auth, return_format='csv'):\n        \"\"\"run query and return as csv, need to add more formats here\"\"\"\n        try:\n            query_run_url = self.api_endpoint + '/queries/{0}/run/{1}'.format(query_id,return_format)\n            #r = requests.get(query_run_url, headers={'Authorization': \"token \" + json_auth})\n            r = requests.get(query_run_url + '?' + 'access_token=' + json_auth)\n            return r.text\n        except requests.exceptions.RequestException as e:\n            logger.error(e)\n            \n    def run_query_slug(self, query_slug, json_auth):\n        \"\"\"run query and return as csv, need to add more formats here\"\"\"\n        try:\n            query_slug_run_url = self.api_endpoint + '/queries/slug/{0}'.format(query_slug)\n            #r = requests.get(query_run_url, headers={'Authorization': \"token \" + json_auth})\n            r = requests.get(query_slug_run_url + '?' + 'access_token=' + json_auth)\n            qid=json.loads(r.text)[\"id\"]\n            print(\"Query_id: \" + str(qid))\n            return LookerAPI.run_query(self, qid, json_auth)\n        except requests.exceptions.RequestException as e:\n            logger.error(e)"
        }, 
        {
            "source": "## Test  LookerAPI calls \n\nTest out API calls on the demonew looker instance! Modify the look ID, Query ID,  or the Query Slug you'd like results from. \n\nIn order to make an API Call, first establish a connection with the Looker Application. Establishing a connection returns a token, that will be used in any subsequent API calls. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Token:Ns4xcndk42nR4XZcSr3wKPNdPVRbfRT3jNy5XyRq\nTrip Trip Count\n286858\n\nQuery_id: 175\nTrip Trip Count\n286858\n\n"
                }
            ], 
            "source": "# Set API credentials for IBM Looker Instance\napi_endpoint = 'https://ibm-wdai.looker.com:19999/api/3.0/'\nlogin_url = 'https://ibm-wdai.looker.com:19999/login'\nclient_id = 'vdRvj8tp8qSC4Yyt3ttj'\nclient_secret = 'gVYw9wQXY6n85fJCqyf5ZBqc'\ndemo = LookerAPI(api_endpoint, client_id, client_secret, login_url)\n\n# Login to Instance - Return token to be used in subsequent calls.\njson_auth = demo.login()\n\nprint('Token:' + json_auth)\n\n# Get Results of Look and Query ID (QID)\nlook_data = demo.run_look(4,json_auth)\nprint(look_data)\n\n# # Get Results of Look by passing in the Query Slug. The slug can be found in the URL. \nquery_response = demo.run_query_slug(\"BeQfwKO07hwbMBqtwoyXtU\", json_auth)\nprint(query_response)\n\n# Convert Query Response from CSV to a dateframe\nrecords = pd.read_csv(StringIO(query_response))\n# records.head(5)\n"
        }, 
        {
            "source": "\nSo far we've sucessfully established a connection to Looker that allows us to pull in modeled data from the Looker explore page or a saved look. \n\nLets take that data and use it to build a predictive Model. \n\n# Example 1\n\nLet's see how different Weather factors for a particular day affect the number of trips taken. \nHow does the temperature or the humidity on any given day affect the average trip length?\n\n### Step 1:\nPass in Data from Looker via API calls\nAn Example Query might look like this -- \nhttps://ibm-wdai.looker.com/looks/5\n\nTemperature: cJhH3ne0pJVL0yJ4mfi27v or https://ibm-wdai.looker.com/looks/5 <br/>\nHumidity: ICgSezXao5VNYvJJioWWA2 or https://ibm-wdai.looker.com/looks/6 <br/>\nTemperature + Humidity: 1dcKd5LCfeSo72onWGlCJv or https://ibm-wdai.looker.com/looks/7 <br/>\n\n\n### Step 2: \nBuild a Predictive Model (In our example we use a Linear Regression model).\nOur regression model uses the Stats Library and prints out the coefficients, Rsquare, P-value, standard deviation, etc... <br/>\n```\nsmf.ols(formula='Y ~ X', data=data).fit()\n```\n\n### Step 3:\nIterate through the Model till you are satisfied with the R2 score. Pass in different fields from Looker via the query slug to predict trip time based on different factors. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Query_id: 188\n  Trip Start Date  Weather Temperature (F)  Weather Humidity  Trip Trip Count\n0      2016-08-31                       65                77              319\n1      2016-08-30                       64                69              375\n2      2016-08-29                       68                65              369\n3      2016-08-28                       68                65              392\n4      2016-08-27                       66                65              333\nColumns: ['Trip Start Date', 'Weather Temperature (F)', 'Weather Humidity', 'Trip Trip Count']\nX Values: ['Weather Temperature (F)', 'Weather Humidity']\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.686\nModel:                            OLS   Adj. R-squared:                  0.685\nMethod:                 Least Squares   F-statistic:                     541.7\nDate:                Mon, 19 Mar 2018   Prob (F-statistic):          1.80e-125\nTime:                        18:23:03   Log-Likelihood:                -2954.2\nNo. Observations:                 499   AIC:                             5914.\nDf Residuals:                     496   BIC:                             5927.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------\nIntercept    222.3647     59.537      3.735      0.000       105.389   339.340\nX[0]           7.7406      0.564     13.721      0.000         6.632     8.849\nX[1]          -4.6283      0.453    -10.225      0.000        -5.518    -3.739\n==============================================================================\nOmnibus:                       21.106   Durbin-Watson:                   1.051\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               42.734\nSkew:                           0.224   Prob(JB):                     5.25e-10\nKurtosis:                       4.362   Cond. No.                     1.32e+03\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.32e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n['Intercept', 'X[0]', 'X[1]', 'Predictor']\n"
                }, 
                {
                    "execution_count": 5, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Intercept</th>\n      <th>X0</th>\n      <th>X1</th>\n      <th>Predictor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>222.364652</td>\n      <td>7.74059</td>\n      <td>-4.628307</td>\n      <td>['Weather Temperature (F)', 'Weather Humidity']</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "    Intercept       X0        X1  \\\n0  222.364652  7.74059 -4.628307   \n\n                                         Predictor  \n0  ['Weather Temperature (F)', 'Weather Humidity']  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# Import Stats packages \nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport re\n\n# Step 1\n# Set API Credentials for Demo Looker Instance and make an API call to get query results. \napi_endpoint = 'https://ibm-wdai.looker.com:19999/api/3.0/'\nlogin_url = 'https://ibm-wdai.looker.com:19999/login'\nclient_id = 'vdRvj8tp8qSC4Yyt3ttj'\nclient_secret = 'gVYw9wQXY6n85fJCqyf5ZBqc'\n\ndemo = LookerAPI(api_endpoint, client_id, client_secret, login_url)\n\n# Login to Instance - Return token to be used in subsequent calls.\n# Run a query with date, temp, humidity, and trip count to use to build regression\n# https://ibm-wdai.looker.com/explore/bike_trips/trip?qid=1dcKd5LCfeSo72onWGlCJv\ntry:\n    json_auth = demo.login()\n    query_response = demo.run_query_slug(\"1dcKd5LCfeSo72onWGlCJv\", json_auth)\nexcept:\n    logger.warn(\"exception logging in and/or running query!\")\n    \n# Convert Query Response from CSV to a dateframe\nrecords = pd.read_csv(StringIO(query_response))\ndata = pd.DataFrame(records)\n\nprint(data.head())\n\n# Step 2\n# Set Predictors and Responses\ntry:\n    column_list = list(data)\n    if(len(column_list) > 2 or len(column_list) < 6):\n        print(\"Columns: \" + str(column_list))\nexcept:\n    logger.error(\"Your Query doesn't seem right. Recreate Query in Looker.\")\n        \n\n# Last Item on the List = Dependent variable, Remaining items = part of Predictor\nY = data[column_list.pop()] # Response ---> Average Trip Duration in Minutes\ncolumn_list.pop(0)\nX = data[column_list] # Predictor ---> All remaining columns are used to predict the model. \nprint(\"X Values: \" + str(column_list))\n\n# Apply a Fitted Model to the dateframe\nmodel = smf.ols(formula='Y ~ X', data=data).fit()\nprint(model.summary())\n\n# Regression Coefficients of the Model are stored in the Params field\ntrip_count_df = pd.DataFrame(model.params)\ntrip_count_df = trip_count_df.T\ntrip_count_df[\"Predictor\"] = str(column_list)\n\nprint(list(trip_count_df))\ncolumns_list = [re.sub('[^A-Za-z0-9]+', '', col) for col in list(trip_count_df)]\n# colnames(trip_time_df) <- cols\ntrip_count_df.columns = columns_list\n\ntrip_count_df.head()\n\n\n\n\n# Step 3\n# Rerun Steps 1 and 2 with different parameters if necessary\n"
        }, 
        {
            "source": "Interpreting the model:\nThe above graphic describes our model. We can see that: \n\n1. A 1 degree increase in temperature leads to nearly 8 (7.74059) additional trips\n2. Temperature and Humidity has a significant effect on trip count (P > | t| is equal to 0)\n3. The R^2 of our basic model is .686 (The model explains 68.6% of the variability in trip count) the AIC is 5914 (We will use this to compare to our models containing other variables)\n\n\nStep 4: \nLet's persist our predictive model results back to the database. In this example we use the credentials from our Db2 connections in DSX and the ingest.Connectors and pyspark.sql packages to convert the dateframe to a spark dataframe and load to a new table called 'Trip Count Prediction'.\n\nWe can leverage the connections feature of Watson Knowledge Studio to automatically include connection credentials for our connected data sources.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 38, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 39, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "from ingest.Connectors import Connectors\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\ndashdbsaveoption = {\nConnectors.DASHDB.HOST : credentials[\"host\"],\nConnectors.DASHDB.DATABASE : credentials[\"database\"],\nConnectors.DASHDB.USERNAME : credentials[\"username\"],\nConnectors.DASHDB.PASSWORD : credentials[\"password\"],\nConnectors.DASHDB.TARGET_TABLE_NAME : 'BIKE_TRIPS.TRIP_COUNT_PREDICTION',\nConnectors.DASHDB.TARGET_TABLE_ACTION : 'replace',\nConnectors.DASHDB.TARGET_WRITE_MODE : 'merge' }\n\ntrip_count_df2 = sqlContext.createDataFrame(trip_count_df)\nNewdashDBDF = trip_count_df2.write.format(\"com.ibm.spark.discover\").options(**dashdbsaveoption).save()"
        }, 
        {
            "source": "Now that we have our predictive model coefficients, Let's pull it up in Looker and calculate how our actual trip times compared to predictive trip times. \n\nWe do this by joining creating a view called 'trip_count_prediction' that references our new table. We join those coefficients into our existing model and Trip explore.\n\n\n### Step 5:\n\nJoin in predictions table in Looker and create fields to predict the Average trip time based on Weather factors. \n\n```\nexplore: trip {\n  join: trip_count_prediction {\n    type: cross\n    relationship: many_to_one\n  }\n```\n\n**Predictive Fields **\nOur predictive measure follows the linear regression model that we used and pulls in the coefficients and intercept that we just persisted back in to the DB.\n\n**Multiple Linear Regression Model:**\n$y = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$ <br/>\nEach $x$ represents a different feature, and each feature has its own coefficient. <br/>\nIn our case: <br/>\n$y = \\beta_0 + \\beta_1 \\times Temperature + \\beta_2 \\times Humidity + \\beta_3 \\times Other Factors$\n\nWe calculate the trip count by creating a measure that calculates the regression function and using our coefficients and the temparature and humidity values. \n```\n  measure: trip_count_prediction{\n    type: average\n    sql:  (${trip_count_prediction.x0} * ${weather.temperature}) + \n          (${trip_count_prediction.x1} * ${weather.humidity}) + \n          ${trip_count_prediction.intercept};;\n    value_format_name: decimal_1\n    view_label: \"Trip Count Prediction\"\n  }\n```\n\nLet's compare our actual average trip times vs. those predicted by our model: <br/>\nData Table/Viz: https://ibm-wdai.looker.com/looks/8 <br/>\n\n\nLets take this a step further and use our model to forecast average trip times/lengths for future dates based on the same factors we've outlined above (the temperature and the humidity).\n\nWe have a seperate notebook that runs daily and pulls the 7 day weather forecast and uploads to a separate table in our Db2 warehouse so we can make future predictions. \n\nYou can access it here: https://dataplatform.ibm.com/analytics/notebooks/v2/3b0de617-210f-4700-9105-9157c2bb8a7a/view?access_token=292635bee35207582e6adceea010e81b1e06386651832e8340e52e299f6920bf\n\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Step 6: \nFuture Predictions: Now that we have the weather forecast for the upcoming week, lets try and predict the trip count for the upcoming week. \n\nLooker Data/Visualization: https://ibm-wdai.looker.com/looks/14\n\n\nWhile this information is useful by itself, we could forecast other factors (like average trip time, number of passholders, etc...) and use our model to forecast revenue for the upcoming week.  \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Example 2\n\nLet's predict the overflow rate (number of bikes taken from the station - number of bikes docked at the station) at certain stations on any given day based on weather conditions like Temperature. \n\n### API Call\nLets start by pulling in data from a particular start station (by Temperature)<br/>\nhttps://ibm-wdai.looker.com/looks/10\nand an end station (by Temperature) <br/>\nhttps://ibm-wdai.looker.com/looks/11\n\n\n### Model\nLet's build a Linear regression model that predicts the number of trips from a station and to a station. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Query_id: 232\nQuery_id: 228\n  Trip Start Date Start Station Station ID  Weather Temperature (F)  \\\n0      2016-08-31                   SLU-19                       65   \n1      2016-08-31                   SLU-07                       65   \n2      2016-08-31                    BT-04                       65   \n3      2016-08-31                    UW-04                       65   \n4      2016-08-31                    CH-02                       65   \n\n   Trip Trip Count  \n0               10  \n1                4  \n2                8  \n3                3  \n4                7  \n  Trip Start Date Trip To Station ID  Weather Temperature (F)  Trip Trip Count\n0      2016-08-31             SLU-04                       65               15\n1      2016-08-31              UW-02                       65                1\n2      2016-08-31             DPD-03                       65                2\n3      2016-08-31              EL-01                       65                6\n4      2016-08-31             CBD-05                       65                4\nColumns: ['Trip Start Date', 'Start Station Station ID', 'Weather Temperature (F)', 'Trip Trip Count']\nColumns: ['Trip Start Date', 'Trip To Station ID', 'Weather Temperature (F)', 'Trip Trip Count']\n  bike_station  start_intercept  start_slope\n0        BT-01        16.617737    -0.010712\n1        BT-03         6.156569     0.053592\n2        BT-04         4.954260     0.039200\n3        BT-05        -0.887119     0.131580\n4       CBD-03         3.264151     0.031925\n  bike_station  end_intercept  end_slope\n0        BT-01     -27.794702   0.556291\n1        BT-03       6.553281   0.003612\n2        BT-04       1.500763   0.125191\n3        BT-05     -10.845802   0.288550\n4       CBD-03      25.900060  -0.311860\n"
                }, 
                {
                    "execution_count": 36, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bike_station</th>\n      <th>start_intercept</th>\n      <th>start_slope</th>\n      <th>end_intercept</th>\n      <th>end_slope</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BT-01</td>\n      <td>16.617737</td>\n      <td>-0.010712</td>\n      <td>-27.794702</td>\n      <td>0.556291</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BT-03</td>\n      <td>6.156569</td>\n      <td>0.053592</td>\n      <td>6.553281</td>\n      <td>0.003612</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BT-04</td>\n      <td>4.954260</td>\n      <td>0.039200</td>\n      <td>1.500763</td>\n      <td>0.125191</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>BT-05</td>\n      <td>-0.887119</td>\n      <td>0.131580</td>\n      <td>-10.845802</td>\n      <td>0.288550</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CBD-03</td>\n      <td>3.264151</td>\n      <td>0.031925</td>\n      <td>25.900060</td>\n      <td>-0.311860</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "  bike_station  start_intercept  start_slope  end_intercept  end_slope\n0        BT-01        16.617737    -0.010712     -27.794702   0.556291\n1        BT-03         6.156569     0.053592       6.553281   0.003612\n2        BT-04         4.954260     0.039200       1.500763   0.125191\n3        BT-05        -0.887119     0.131580     -10.845802   0.288550\n4       CBD-03         3.264151     0.031925      25.900060  -0.311860"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "#### Simple Linear Regression w/ Start Stations \n\n# Import Stats packages \nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport re\n\n# Step 1\n# Set API Credentials for Demo Looker Instance and make an API call to get query results. \napi_endpoint = 'https://ibm-wdai.looker.com:19999/api/3.0/'\nlogin_url = 'https://ibm-wdai.looker.com:19999/login'\nclient_id = 'vdRvj8tp8qSC4Yyt3ttj'\nclient_secret = 'gVYw9wQXY6n85fJCqyf5ZBqc'\n\ndemo = LookerAPI(api_endpoint, client_id, client_secret, login_url)\n# Login to Instance - Return token to be used in subsequent calls.\ntry:\n    json_auth = demo.login()\n    start_station = demo.run_query_slug(\"KDE803l02xeZKIVNdUeuwp\", json_auth)\n    end_station = demo.run_query_slug(\"5Rptglr5g0uiFyVFLkEDfP\", json_auth)\nexcept:\n    logger.warn(\"exception logging in and/or running query!\")\n    \n# Convert Query Response from CSV to a dateframe\nstart_station = pd.read_csv(StringIO(start_station))\nstart_data = pd.DataFrame(start_station)\nprint(start_data.head())\n\nend_station = pd.read_csv(StringIO(end_station))\nend_data = pd.DataFrame(end_station)\nprint(end_data.head())\n\n\n# Step 2\n# Calculate best fit line for trips starting from Station\ntry:\n    column_list = list(start_data)\n    if(len(column_list) > 2 or len(column_list) < 6):\n        print(\"Columns: \" + str(column_list))\n    \n    column_list = list(end_data)\n    if(len(column_list) > 2 or len(column_list) < 6):\n        print(\"Columns: \" + str(column_list))\nexcept:\n    logger.error(\"Your Query doesn't seem right. Recreate Query in Looker.\")\n\n\n## L-reg by Station using StatsModel\nl_reg_start = []\ndf_group = start_data.groupby('Start Station Station ID')\nfor station, group in start_data.groupby('Start Station Station ID'):\n    new_df= df_group.get_group(station)\n    x=new_df['Weather Temperature (F)']\n    y=new_df['Trip Trip Count']\n    lm = smf.ols(formula='y ~ x', data=new_df).fit()\n    l_reg_start.append({\n        'bike_station': station, \n        'start_slope': lm.params.x,\n        'start_intercept': lm.params.Intercept\n    })\n\nl_reg_start_df = pd.DataFrame(l_reg_start)  \n\nprint(l_reg_start_df.head())\n\n\n# Step 3\n\n## L-reg by End Station using StatsModel\nl_reg_end = []\ndf_group = end_data.groupby('Trip To Station ID')\nfor station, group in end_data.groupby('Trip To Station ID'):\n    new_df= df_group.get_group(station)\n    x=new_df['Weather Temperature (F)']\n    y=new_df['Trip Trip Count']\n    lm = smf.ols(formula='y ~ x', data=new_df).fit()\n    l_reg_end.append({\n        'bike_station': station, \n        'end_slope': lm.params.x,\n        'end_intercept': lm.params.Intercept\n        \n    })\nl_reg_end_df = pd.DataFrame(l_reg_end)  \nprint(l_reg_end_df.head())\n\n\n# Step 4\n# Combine Start and End predictions by Bike Station\nl_reg_df = pd.merge(l_reg_start_df, l_reg_end_df, on='bike_station')\nl_reg_df.head()\n\n"
        }, 
        {
            "execution_count": 40, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from ingest.Connectors import Connectors\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\ndashdbsaveoption = {\nConnectors.DASHDB.HOST : credentials[\"host\"],\nConnectors.DASHDB.DATABASE : credentials[\"database\"],\nConnectors.DASHDB.USERNAME : credentials[\"username\"],\nConnectors.DASHDB.PASSWORD : credentials[\"password\"],\nConnectors.DASHDB.TARGET_TABLE_NAME : 'BIKE_TRIPS.STATION_REGRESSION',\nConnectors.DASHDB.TARGET_TABLE_ACTION : 'replace',\nConnectors.DASHDB.TARGET_WRITE_MODE : 'merge' }\n\nl_reg_df2 = sqlContext.createDataFrame(l_reg_df)\nNewdashDBDF = l_reg_df2.write.format(\"com.ibm.spark.discover\").options(**dashdbsaveoption).save()"
        }, 
        {
            "source": "Now that we have our predictive model, Let's pull it up in Looker and calculate how Trips per station. \n\n\nJoin in *station prediction* table in LookMl to our Trips Explore and create fields to predict trips taken based on Temperature. This quickly allows us to explore the data with predicted station counts in place. \n\n```\nexplore: trip {\n  join: station_prediction  {\n    type:  left_outer\n    relationship: one_to_one\n    sql_on:  ${trip.from_station_id} = ${station_prediction.bike_station} ;;\n  }\n```\n\n**Predictive Fields **\nOur predictive measure follows the linear regression model that we used and pulls in the coefficients and intercept that we just persisted back in to the DB.\n\n**Linear Regression Model:** <br/>\n$y = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$ <br/>\nEach $x$ represents a different feature, and each feature has its own coefficient. <br/>\nIn our case: <br/>\n$y = \\beta_0 + \\beta_1 \\times Temperature$\n\nWe build these calculations in our LookML model \n\n```\n  measure:  start_predictions {\n    type:  average\n    sql: (${start_slope} * ${weather.temperature} ) +  ${start_intercept};;\n    value_format_name: decimal_1\n  }\n\n  measure:  end_predictions {\n    type:  average\n    sql: (${end_slope} * ${weather.temperature} ) +  ${end_intercept};;\n    value_format_name: decimal_1\n  }\n\n  measure:  predicted_station_overflow {\n    type: number\n    sql:  ${end_predictions} - ${start_predictions} ;;\n    value_format_name: decimal_1\n  }\n  \n  \n  \n```\n\n\n\n** Reports and Dashboards in Looker:** <br/>\n\nLet's use our model to forecast trips taken from a station for future dates based on the temperature:\nhttps://ibm-wdai.looker.com/looks/9\n\nLet's bring in additional data about stations and plot overflow by Location for a particular date: \nLet's use our model to forecast trips taken from a station for future dates based on the temperature:\nhttps://ibm-wdai.looker.com/looks/12\n\nPulling it all together we build a station manager dashboard, so a manager can take immediate action based on the predicted overflow at stations in their region: https://ibm-wdai.looker.com/dashboards/7\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}
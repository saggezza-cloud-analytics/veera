{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# Build and deploy an SMS spam classifer with Watson Mahine Learning\n**An Introduction to the Watson Machine Learning Python Client** <br> <br>\nThis notebook will show you how to build and deploy an SMS Spam Classifer with Watson Machine Learning and IBM Watson Studio. <br> We will use the new [Watson Machine Learning API Client for Python](http://wml-api-pyclient.mybluemix.net/) which is avialable on `PyPi`. \n______\nThis notebook was tested in a `Python 3.5` Environment. \nThis notebook can be used as a companion to another [tutorial on our blog](https://medium.com/@adammassachi/dsx-hybrid-mode-91b580450c5b).  <br>\n\n\n## Table of Contents\n1. [Load data](#load)\n2. [Build model](#build)\n3. [Save and deploy](#save)\n4. [Make API requests](#api)\n\n_____\n\n## 1. Load data <a id=\"load\"></a>\nFirst, install the Watson Machine Learning library via `pip` if you have not yet installed it. <br> We will use this library to communicate with Watson Machine Learning. The `python client` allows anyone with a Watson Machine Learning instance to programmatically save, load, and deploy models, among other tasks. ", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "!pip install watson-machine-learning-client --upgrade", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "The data we are looking to classify are SMS Messages which have been labeled `spam` or `ham`. You can find the data in the community. You can either download the data set to your environment and add it to your project, or you can read the data directly into a data frame as shown below. For more details on loading and accessing data, see [Load and access data in a notebook](https://datascience.ibm.com/docs/content/analyze-data/load-and-access-data.html?context=analytics).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<!--comment: Simon, here you need to tell them how to access data. They can either (1) go to the community and download the csv with the 'add data' link above, \nor (2) import the csv directly as in the cell below. Either way, we should be explicit...  I'm trying to find a good example, and frankly, we don't have a really good one.\nHaving said that, you can have a look at https://github.com/IBMDataScience/sample-notebooks/blob/ec37c3f56f33cf8aa85c00e9081f13012ffd8fd5/Cloud/IPYNB/Watson%2Bconversation%2Bservice.ipynb\n-->", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "source": "import pandas as pd\ndf = pd.read_csv(\"https://dataplatform.ibm.com/exchange-api/v1/entries/e39fb7848165baca7fc0395025ba4e48/data?accessKey=36100ef896c27e41fdfc4a3029071d50\")", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Our first step will be converting the string label into a numeric representation. <br> \nWe can use a `pandas.Series method`,  `factorize()[0]`, to convert strings into numeric factors.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "source": "df = df[df.columns[:2]]\ndf.columns = ['ham', 'text']\ndf['label'] = df.ham.factorize()[0]\ndf['text'] = df.text.apply(lambda x: x.lower())", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "source": "df.head()", 
            "outputs": [
                {
                    "execution_count": 4, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "    ham                                               text  label\n0   ham  go until jurong point, crazy.. available only ...      0\n1   ham                      ok lar... joking wif u oni...      0\n2  spam  free entry in 2 a wkly comp to win fa cup fina...      1\n3   ham  u dun say so early hor... u c already then say...      0\n4   ham  nah i don't think he goes to usf, he lives aro...      0", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ham</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>go until jurong point, crazy.. available only ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>ok lar... joking wif u oni...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>u dun say so early hor... u c already then say...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>nah i don't think he goes to usf, he lives aro...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "metadata": {}
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"build\"></a>\n## 2. Build a model \nWe will use `scikit-learn` to create a `Naive Bayes` model. <br>We will use the `HashingVectorizer`, which converts the SMS\u2019 text into a matrix representation suitable for modeling.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "source": "from sklearn.feature_extraction.text import HashingVectorizer\nvectorizer = HashingVectorizer(n_features=5000, stop_words='english', non_negative=True)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "We need to connect the output of the `vectorizer` to the input of the model. We will use `Multinomial Naive Bayes`, a Naive Bayes classifier which works well with the representation of our features\u200a \u2014\u200a integer representations of the word frequencies.\n\n\nNext, we will use `train_test_split` in order to divide the data into `testing` and `training` sets so that we can evaluate the performance of the model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "source": "from sklearn.cross_validation import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df['text'], df['label'], random_state=0)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "Next, we need to transform the text and fit the model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "source": "# first transform the text data\ntransformed_x = vectorizer.fit_transform(x_train)\n\n# import the modules and fit\nfrom sklearn.naive_bayes import MultinomialNB\nbn = MultinomialNB().fit(transformed_x, y_train)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n  \" in version 0.21.\", DeprecationWarning)\n/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n  \" in version 0.21.\", DeprecationWarning)\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "We\u2019ve got a fit model in `bn`. Let\u2019s evaluate the performance on the test data after creating the pipeline.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "source": "# make a pipe\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline(vectorizer, bn)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "The pipe will sequentially transform the data according to the transformers specified, terminating in what scikit-learn calls an estimator. <br>Then, we can call predict or score, and so on.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "source": "pipe.predict_proba([\"URGENT! You have built a model - scroll down to see more\"])", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n  \" in version 0.21.\", DeprecationWarning)\n"
                }, 
                {
                    "execution_count": 9, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "array([[ 0.79965197,  0.20034803]])"
                    }, 
                    "metadata": {}
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "Let's score.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "source": "pipe.score(x_test, y_test)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n  \" in version 0.21.\", DeprecationWarning)\n"
                }, 
                {
                    "execution_count": 10, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "0.96197991391678628"
                    }, 
                    "metadata": {}
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "`96% accuracy`, not bad. You can experiment with different numbers of features and vectorizers for your model. You can also create other features that are not captured by the vectorizer, such as the length of the message. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"save\"></a>\n## 3. Save and deploy\nAfter creating a model, you might want to make use of its predictions later. In order to do this, we will persist models with Watson Machine Learning. With WML, you can easily `save` and `deploy` models, among other powerful features. `Saving` a model makes this model portable -- as long as you can connect to WML, you can load your saved models into your environment. `Deploying` a model exposes the predictive capacity of the model as an API endpoint, which you can consume in applications, for example. \n\nUse the client to save your model to the WML Repository. From there, you can load and deploy models as well. If you don't already have a WML account, you can get more details [here](https://dataplatform.ibm.com/docs/content/analyze-data/ml-setup.html?audience=wdp&context=analytics).\n\nFirst, we will import the library and specify our credentials. If you don't know where to find your credentials, they are available to you both in the Watson Studio Project and in IBM Cloud. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n\nwml_credentials = {\n  \"username\": \"\",\n  \"password\": \"\",\n  \"instance_id\": \"\",\n  \"url\": \"\"\n}\n\nclient = WatsonMachineLearningAPIClient(wml_credentials)", 
            "outputs": [], 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "source": "We have connected to our WML Repository using the python package and our credentials. Publishing the model will save the model to our repository. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "source": "metadata = {\n            client.repository.ModelMetaNames.NAME: 'SPAM_MODEL',\n            client.repository.ModelMetaNames.FRAMEWORK_NAME: 'scikit-learn',\n            client.repository.ModelMetaNames.RUNTIME_NAME: 'python',\n            client.repository.ModelMetaNames.RUNTIME_VERSION: '3.5'\n}", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "source": "# publish model \npublished_model_details = client.repository.store_model(model=pipe, meta_props=metadata, training_data=pd.DataFrame(df['text']), training_target=df['label'])", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now that we have saved the model to the repository, we can load it into a python object using it's `uid`. First, let's look at the code.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "source": "# get my model ide\nguid = client.repository.get_model_uid(published_model_details)\nguid", 
            "outputs": [
                {
                    "execution_count": 14, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "'0b4319bc-6eca-4aa7-8642-0b25f46d82fc'"
                    }, 
                    "metadata": {}
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "Load the model using its id and the `repository.load()` function. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "source": "mod = client.repository.load(guid)\ntype(mod)", 
            "outputs": [
                {
                    "execution_count": 15, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "sklearn.pipeline.Pipeline"
                    }, 
                    "metadata": {}
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "Notice the type is an `sklearn` model, just as we created. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "source": "# verify that the loaded model returns the same predictions as the model we developed in our environment. \nmod.predict_proba([\"URGENT! You have built a model - scroll down to see more\"])", 
            "outputs": [
                {
                    "execution_count": 16, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "array([[ 0.79965197,  0.20034803]])"
                    }, 
                    "metadata": {}
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"api\"></a>\n## 4. Test the API\nNow that we have successfully saved and loaded the model, we can create an API endpoint and make requests. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "source": "scoring_endpoint = client.deployments.create(name=\"SPAM-NEW\", model_uid=guid)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: '0b4319bc-6eca-4aa7-8642-0b25f46d82fc' started\n\n#######################################################################################\n\n\nINITIALIZING\nDEPLOY_SUCCESS\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='ba19aaf1-1ee1-4c95-8adf-94989a080506'\n------------------------------------------------------------------------------------------------\n\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "source": "scoring_endpoint_url = scoring_endpoint['entity']['scoring_url']", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Let's create some JSON to send. We will use `client.deployments.score(scoring_url, payload)`. [Read our docs](http://wml-api-pyclient.mybluemix.net/) for more details. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "source": "# create a payload\nmy_sms = \"Send me to the API por favor\"\n# list of lists\npayload = {\"fields\": [\"text\"], \"values\": [[my_sms]]}\n\n# make a request\nresponse = client.deployments.score(scoring_url=scoring_endpoint_url, payload=payload)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Let's check out the response.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "source": "response", 
            "outputs": [
                {
                    "execution_count": 20, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "{'fields': ['prediction', 'probability'],\n 'values': [[0, [0.8211305335459812, 0.17886946645401927]]]}"
                    }, 
                    "metadata": {}
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "\n\n____________\n\n### Author\nAdam Massachi is a Data Scientist with the Watson Studio team at IBM. Before IBM, he worked on political campaigns, building and managing large volunteer operations and organizing campaign finance initiatives. Say hello [@adammassach](https://twitter.com/adammassach?lang=en)!\n\nCopyright \u00a9 IBM Corp. 2018. This notebook and its source code are released under the terms of the MIT License.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }
}
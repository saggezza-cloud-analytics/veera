{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "<div style=\"background:#F5F7FA; height:100px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Want to do more?</span><span style=\"border: 1px solid #3d70b2;padding: 15px;float:right;margin-right:40px; color:#3d70b2; \"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n<span style=\"color:#5A6872;\"> Try out this notebook with your free trial of IBM Watson Studio.</span>\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Introduction to Apache Spark lab, part 3: machine learning\n\nIn this notebook you'll learn how to create a model for purchase recommendations using the alternating least squares algorithm of the Apache Spark machine learning library. Machine learning is based on algorithms that can learn from data without relying on rules-based programming.  \n\n\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E\"\n-Tom M. Mitchell\n\nThis notebook uses pySpark, the Python API for Spark. Some knowledge of Python is recommended. This notebook runs on Python 2 with Spark 2.1.\n\nIf you are new to Apache Spark, see the first two parts of this lab: \n - [Introduction to Apache Spark lab, part 1: Basic Concepts](https://apsportal.ibm.com/exchange/public/entry/view/95811fca38af4ccbea8acf8658bedcfc)\n - [Introduction to Apache Spark lab, part 2: Querying data](https://apsportal.ibm.com/exchange/public/entry/view/5ad1c820f57809ddec9a040e37b2bd55)  \n\n## Spark machine learning library\n<a href=\"http://spark.apache.org/docs/latest/mllib-guide.html\" target=\"_blank\" rel=\"noopener noreferrer\">Apache Spark\u2019s machine learning library</a> makes practical machine learning scalable and easy. The library consists of common machine learning algorithms and utilities, including classification, regression, clustering, collaborative filtering (this notebook!), dimensionality reduction, lower-level optimization primitives, and higher-level pipeline APIs.\n\nThe library has two packages:\n- spark.mllib contains the original API that handles data in RDDs. It's in maintenance mode, but fully supported.\n- spark.ml contains a newer API for constructing ML pipelines. It handles data in DataFrames. It's being actively enhanced.\n\n## Alternating least squares algorithm\nThe alternating least squares (ALS) algorithm provides collaborative filtering between customers and products to find products that the customers might like, based on their previous purchases or ratings.\n\nThe ALS algorithm creates a matrix of all customers versus all products. Most cells in the matrix are empty, which means the customer hasn't bought that product. The ALS algorithm then fills in the probability of customers buying products that they haven't bought yet, based on similarities between customer purchases and similarities between products. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between fixing the customer factors and solving for product factors and fixing the product factors and solving for customer factors.\n\nYou don't, however, need to understand how the ALS algorithm works to use it! Spark machine learning algorithms have default values that work well in most cases.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Table of contents\n\n1. [Get the data](#getdata)<br>\n2. [Prepare and shape the data](#prepare)<br>\n    2.1 [Format the data](#prepare1)<br>\n    2.2 [Clean the data](#prepare2)<br>\n    2.3 [Create a DataFrame](#prepare3)<br>\n    2.4 [Remove unneeded columns](#prepare4)<br>\n3. [Split the data into three sets](#split)<br>\n4. [Build recommendation models](#model)<br>\n5. [Test the models](#test)<br>\n    5.1 [Clean the cross validation data set](#test1)<br>\n    5.2 [Run the models on the cross validation data set](#test2)<br>\n    5.3 [Calculate the accuracy for each model](#test3)<br>\n    5.4 [Confirm the best model](#test4)<br>\n6. [Implement the mode](#implement)<br>\n    6.1 [Create a DataFrame for the customer and all products](#implement1)<br>\n    6.2 [Rate each product](#implement2)<br>\n    6.3 [Find the top recommendations](#implement3)<br>\n    6.4 [Compare purchased and recommended products](#implement4)<br>\n7. [Summary and next steps](#summary)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"getdata\"></a>\n## 1. Get the data \nThe data set contains the transactions of an online retailer of gift items for the period from 01/12/2010 to 09/12/2011. Many of the customers are wholesalers.\n\nYou'll be using a slightly modified version of this <a href=\"http://archive.ics.uci.edu/ml/datasets/Online+Retail](http://archive.ics.uci.edu/ml/datasets/Online+Retail\" target=\"_blank\" rel=\"noopener noreferrer\">data set</a>.  \n\nHere's a glimpse of the data:\n\n<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/FullFile.png' width=\"80%\" height=\"80%\"></img>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Download the CSV version of the data set, from which commas in the product descriptions are removed:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "source": "!rm 'OnlineRetail.csv.gz' -f\n!wget https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2018-03-05 02:47:58--  https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7483128 (7.1M) [application/octet-stream]\nSaving to: \u2018OnlineRetail.csv.gz\u2019\n\n100%[======================================>] 7,483,128   --.-K/s   in 0.1s    \n\n2018-03-05 02:48:00 (59.3 MB/s) - \u2018OnlineRetail.csv.gz\u2019 saved [7483128/7483128]\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "Put the data into an RDD and print the first 5 rows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "source": "loadRetailData = sc.textFile(\"OnlineRetail.csv.gz\")\n\nfor row in loadRetailData.take(5):\n    print row", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/10 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/10 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/10 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/10 8:26,3.39,17850,United Kingdom\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "Each row in the RDD is a string that correlates to a line in the CSV file.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"prepare\"></a>\n## 2. Prepare and shape the data\n\nIt's been said that preparing and shaping data is 80% of a data scientist's job. Having the right data in the right format is critical for getting accurate results.\n\nTo get the data ready, complete these tasks:\n\n1. [Format the data](#prepare1)\n1. [Clean the data](#prepare2)\n1. [Create a DataFrame](#prepare3)\n1. [Remove unneeded columns](#prepare4)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"prepare1\"></a>\n### 2.1 Format the data\nRemove the header from the RDD and split the string in each row with a comma:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "source": "header = loadRetailData.first()\nloadRetailData = loadRetailData.filter(lambda line: line != header).\\\n                            map(lambda l: l.split(\",\"))\n\nfor row in loadRetailData.take(5):\n    print row", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[u'536365', u'85123A', u'WHITE HANGING HEART T-LIGHT HOLDER', u'6', u'12/1/10 8:26', u'2.55', u'17850', u'United Kingdom']\n[u'536365', u'71053', u'WHITE METAL LANTERN', u'6', u'12/1/10 8:26', u'3.39', u'17850', u'United Kingdom']\n[u'536365', u'84406B', u'CREAM CUPID HEARTS COAT HANGER', u'8', u'12/1/10 8:26', u'2.75', u'17850', u'United Kingdom']\n[u'536365', u'84029G', u'KNITTED UNION FLAG HOT WATER BOTTLE', u'6', u'12/1/10 8:26', u'3.39', u'17850', u'United Kingdom']\n[u'536365', u'84029E', u'RED WOOLLY HOTTIE WHITE HEART.', u'6', u'12/1/10 8:26', u'3.39', u'17850', u'United Kingdom']\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"prepare2\"></a>\n### 2.2 Clean the data\nRemove the rows that have incomplete data. Keep only the rows that meet the following criteria:\n - The purchase quantity is greater than 0 \n - The customer ID not equal to 0 \n - The stock code is not blank after you remove non-numeric characters", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "source": "import re\n\nloadRetailData = loadRetailData.filter(lambda l: int(l[3]) > 0\\\n                                and len(re.sub(\"\\D\", \"\", l[1])) != 0 \\\n                                and len(l[6]) != 0)\n\nprint loadRetailData.take(2)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[u'536365', u'85123A', u'WHITE HANGING HEART T-LIGHT HOLDER', u'6', u'12/1/10 8:26', u'2.55', u'17850', u'United Kingdom'], [u'536365', u'71053', u'WHITE METAL LANTERN', u'6', u'12/1/10 8:26', u'3.39', u'17850', u'United Kingdom']]\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"prepare3\"></a>\n### 2.3 Create a DataFrame\n\nFirst, create an SQLContext and map each line to a row: ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "source": "from pyspark.sql import SQLContext, Row\nsqlContext = SQLContext(sc)\n\n#Convert each line to a Row.\nloadRetailData = loadRetailData.map(lambda l: Row(inv=int(l[0]),\\\n                                    stockCode=int(re.sub(\"\\D\", \"\", l[1])),\\\n                                    description=l[2],\\\n                                    quant=int(l[3]),\\\n                                    invDate=l[4],\\\n                                    price=float(l[5]),\\\n                                    custId=int(l[6]),\\\n                                    country=l[7]))", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Create a DataFrame and show the inferred schema:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "source": "retailDf = sqlContext.createDataFrame(loadRetailData)\nprint retailDf.printSchema()", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- country: string (nullable = true)\n |-- custId: long (nullable = true)\n |-- description: string (nullable = true)\n |-- inv: long (nullable = true)\n |-- invDate: string (nullable = true)\n |-- price: double (nullable = true)\n |-- quant: long (nullable = true)\n |-- stockCode: long (nullable = true)\n\nNone\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "Register the DataFrame as a table so that you can run SQL queries on it and show the first two rows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "source": "retailDf.registerTempTable(\"retailPurchases\")\nsqlContext.sql(\"SELECT * FROM retailPurchases limit 2\").toPandas()", 
            "outputs": [
                {
                    "execution_count": 7, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "          country  custId                         description     inv  \\\n0  United Kingdom   17850  WHITE HANGING HEART T-LIGHT HOLDER  536365   \n1  United Kingdom   17850                 WHITE METAL LANTERN  536365   \n\n        invDate  price  quant  stockCode  \n0  12/1/10 8:26   2.55      6      85123  \n1  12/1/10 8:26   3.39      6      71053  ", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>custId</th>\n      <th>description</th>\n      <th>inv</th>\n      <th>invDate</th>\n      <th>price</th>\n      <th>quant</th>\n      <th>stockCode</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>United Kingdom</td>\n      <td>17850</td>\n      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n      <td>536365</td>\n      <td>12/1/10 8:26</td>\n      <td>2.55</td>\n      <td>6</td>\n      <td>85123</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>United Kingdom</td>\n      <td>17850</td>\n      <td>WHITE METAL LANTERN</td>\n      <td>536365</td>\n      <td>12/1/10 8:26</td>\n      <td>3.39</td>\n      <td>6</td>\n      <td>71053</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "metadata": {}
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"prepare4\"></a>\n### 2.4 Remove unneeded columns\nThe only columns you need are `custId`, `stockCode`, and a new column, `purch`, which has a value of 1 to indicate that the customer purchased the product:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "source": "query = \"\"\"\nSELECT \n    custId, stockCode, 1 as purch\nFROM \n    retailPurchases \ngroup \n    by custId, stockCode\"\"\"\nretailDf = sqlContext.sql(query)\nretailDf.registerTempTable(\"retailDf\")\n\nsqlContext.sql(\"select * from retailDf limit 10\").toPandas()", 
            "outputs": [
                {
                    "execution_count": 8, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "   custId  stockCode  purch\n0   18074      22224      1\n1   13705      21889      1\n2   15862      22441      1\n3   15862      21592      1\n4   12838      22739      1\n5   12838      22149      1\n6   14078      22548      1\n7   14078      22423      1\n8   12433      21977      1\n9   14696      84360      1", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>custId</th>\n      <th>stockCode</th>\n      <th>purch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18074</td>\n      <td>22224</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13705</td>\n      <td>21889</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15862</td>\n      <td>22441</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15862</td>\n      <td>21592</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12838</td>\n      <td>22739</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12838</td>\n      <td>22149</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>14078</td>\n      <td>22548</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>14078</td>\n      <td>22423</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>12433</td>\n      <td>21977</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>14696</td>\n      <td>84360</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "metadata": {}
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"split\"></a>\n## 3. Split the data into three sets\nYou'll split the data into three sets: \n - a testing data set (10% of the data)\n - a cross-validation data set (10% of the data)\n - a training data set (80% of the data)\n\nSplit the data randomly and create a DataFrame for each data set:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "source": "testDf, cvDf, trainDf = retailDf.randomSplit([.1,.1,.8],1)\n\nprint \"trainDf count: \", trainDf.count(), \" example: \"\nfor row in trainDf.take(2): print row\nprint\n\nprint \"cvDf count: \", cvDf.count(), \" example: \"\nfor row in cvDf.take(2): print row\nprint\n\nprint \"testDf count: \", testDf.count(), \" example: \"\nfor row in testDf.take(2): print row\nprint", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "trainDf count:  208123  example: \nRow(custId=12359, stockCode=23345, purch=1)\nRow(custId=12363, stockCode=20685, purch=1)\n\ncvDf count:  25876  example: \nRow(custId=12349, stockCode=23545, purch=1)\nRow(custId=12388, stockCode=22960, purch=1)\n\ntestDf count:  26113  example: \nRow(custId=12362, stockCode=22372, purch=1)\nRow(custId=12391, stockCode=20985, purch=1)\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"model\"></a>\n## 4. Build recommendation models\nMachine learning algorithms have standard parameters and hyperparameters. Standard parameters specify data and options. Hyperparameters control the performance of the algorithm.\n\nThe ALS algorithm has these hyperparameters:  \n\n - The `rank` hyperparameter represents the number of features. The default value of `rank` is 10.\n - The `maxIter` hyperparameter represents the number of iterations to run the least squares computation. The default value of `maxIter` is 10.\n\nUse the training DataFrame to train three models with the ALS algorithm with different values for the `rank` and `maxIter` hyperparameters. Assign the `userCol`, `itemCol`, and `ratingCol` parameters to the appropriate data columns. Set the `implicitPrefs` parameter to `true` so that the algorithm can predict latent factors.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "source": "from pyspark.ml.recommendation import ALS\n\nals1 = ALS(rank=3, maxIter=15,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel1 = als1.fit(trainDf)\n\nals2 = ALS(rank=15, maxIter=3,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel2 = als2.fit(trainDf)\n\nals3 = ALS(rank=15, maxIter=15,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel3 = als3.fit(trainDf)\n\nprint \"The models are trained\"", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "The models are trained\n"
                }
            ], 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "source": "<a id=\"test\"></a>\n## 5. Test the models\n\nFirst, test the three models on the cross-validation data set, and then on the testing data set. \n\nYou'll know the model is accurate when the prediction values for products that the customers have already bought are close to 1. \n\n<a id=\"test1\"></a>\n### 5.1 Clean the cross validation data set\n\nRemove any of the customers or products in the cross-validation data set that are not in the training data set:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "source": "from pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import BooleanType\ncustomers = set(trainDf.rdd.map(lambda line: line.custId).collect())\nstock = set(trainDf.rdd.map(lambda line: line.stockCode).collect())\n\nprint cvDf.count()\ncvDf = cvDf.rdd.filter(lambda line: line.stockCode in stock and\\\n                                           line.custId in customers).toDF()\nprint cvDf.count()", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "25876\n25846\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"test2\"></a>\n### 5.2 Run the models on the cross-validation data set\nRun the model with the cross-validation DataFrame by using the `transform` function and print the first two rows of each set of predictions:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "source": "predictions1 = model1.transform(cvDf)\npredictions2 = model2.transform(cvDf)\npredictions3 = model3.transform(cvDf)\n\nprint predictions1.take(2)\nprint predictions2.take(2)\nprint predictions3.take(2)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Row(custId=14606, stockCode=20735, purch=1, prediction=0.03523101285099983), Row(custId=16464, stockCode=20735, purch=1, prediction=0.009832920506596565)]\n[Row(custId=16669, stockCode=20735, purch=1, prediction=0.016950156539678574), Row(custId=14606, stockCode=20735, purch=1, prediction=0.044680818915367126)]\n[Row(custId=16669, stockCode=20735, purch=1, prediction=0.012691554613411427), Row(custId=14606, stockCode=20735, purch=1, prediction=0.1028711274266243)]\n"
                }
            ], 
            "metadata": {
                "scrolled": false
            }
        }, 
        {
            "source": "<a id=\"test3\"></a>\n### 5.3 Calculate the accuracy for each model  \n\nYou'll use the mean squared error calculation to determine accuracy by comparing the prediction values for products to the actual purchase values. Remember that if a customer purchased a product, the value in the `purch` column is 1. The mean squared error calculation measures the average of the squares of the errors between what is estimated and the existing data. The lower the mean squared error value, the more accurate the model. \n\nFor all predictions, subtract the prediction from the actual purchase value (1), square the result, and calculate the mean of all of the squared differences:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "source": "meanSquaredError1 = predictions1.rdd.map(lambda line: (line.purch - line.prediction)**2).mean()\nmeanSquaredError2 = predictions2.rdd.map(lambda line: (line.purch - line.prediction)**2).mean()\nmeanSquaredError3 = predictions3.rdd.map(lambda line: (line.purch - line.prediction)**2).mean()\n    \nprint 'Mean squared error = %.4f for our first model' % meanSquaredError1\nprint 'Mean squared error = %.4f for our second model' % meanSquaredError2\nprint 'Mean squared error = %.4f for our third model' % meanSquaredError3", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Mean squared error = 0.7388 for our first model\nMean squared error = 0.6990 for our second model\nMean squared error = 0.6681 for our third model\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "The third model (model3) has the lowest mean squared error value, so it's the most accurate.\n\nNotice that of the three models, model3 has the highest values for the hyperparameters. At this point you might be tempted to run the model with even higher values for `rank` and `maxIter`. However, you might not get better results. Increasing the values of the hyperparameters increases the time for the model to run. Also, you don't want to overfit the model so that it exactly fits the original data. In that case, you wouldn't get any recommendations! For best results, keep the values of the hyperparameters close to the defaults.\n\n<a id=\"test4\"></a>\n### 5.4 Confirm the best model \n\nNow run model3 on the testing data set to confirm that it's the best model. You want to make sure that the model is not over-matched to the cross-validation data. It's possible for a model to match one subset of the data well but not another. If the values of the mean squared error for the testing data set and the cross-validation data set are close, then you've confirmed that the model works for all the data.\n\nClean the testing data set, run model3 on the testing data set, and calculate the mean squared error:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "source": "filteredTestDf = testDf.rdd.filter(lambda line: line.stockCode in stock and\\\n                                              line.custId in customers).toDF()\npredictions4 = model3.transform(filteredTestDf)\nmeanSquaredError4 = predictions4.rdd.map(lambda line: (line.purch - line.prediction)**2).mean()\n    \nprint 'Mean squared error = %.4f for our best model' % meanSquaredError4", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Mean squared error = 0.6693 for our best model\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "That's pretty close. The model works for all the data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"implement\"></a>\n## 6. Implement the model\n\nUse the best model to predict which products a specific customer might be interested in purchasing.\n\n<a id=\"implement1\"></a>\n### 6.1 Create a DataFrame for the customer and all products \n\nCreate a DataFrame in which each row has the customer ID (15544) and a product ID:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "source": "from pyspark.sql.functions import lit\n\nstock15544 = set(trainDf.filter(trainDf['custId'] == 15544).rdd.map(lambda line: line.stockCode).collect())\n\nuserItems = trainDf.select(\"stockCode\").distinct().\\\n            withColumn('custId', lit(15544)).\\\n            rdd.filter(lambda line: line.stockCode not in stock15544).toDF()\n\nfor row in userItems.take(5):\n    print row.stockCode, row.custId", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "21899 15544\n22429 15544\n22201 15544\n22165 15544\n23019 15544\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"implement2\"></a>\n### 6.2 Rate each product\n\nRun the `transform` function to create a prediction value for each product:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "source": "userItems = model3.transform(userItems)\n\nfor row in userItems.take(5):\n    print row.stockCode, row.custId, row.prediction", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "20735 15544 0.00423703156412\n21220 15544 0.0538208372891\n21700 15544 0.069147542119\n22097 15544 -0.0548023916781\n22223 15544 0.020398337394\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"implement3\"></a>\n### 6.3 Find the top recommendations\n\nPrint the top five product recommendations:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "source": "userItems.registerTempTable(\"predictions\")\nquery = \"select * from predictions order by prediction desc limit 5\"\n\nsqlContext.sql(query).toPandas()", 
            "outputs": [
                {
                    "execution_count": 17, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "   stockCode  custId  prediction\n0      22326   15544    0.529536\n1      21242   15544    0.524287\n2      21559   15544    0.501251\n3      22090   15544    0.490788\n4      22367   15544    0.482872", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stockCode</th>\n      <th>custId</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22326</td>\n      <td>15544</td>\n      <td>0.529536</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21242</td>\n      <td>15544</td>\n      <td>0.524287</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21559</td>\n      <td>15544</td>\n      <td>0.501251</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22090</td>\n      <td>15544</td>\n      <td>0.490788</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22367</td>\n      <td>15544</td>\n      <td>0.482872</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "metadata": {}
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"implement4\"></a>\n### 6.4 Compare purchased and recommended products\n\nHere's a view of the products that customer 15544 bought:\n\n<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/user.png' width=\"80%\" height=\"80%\"></img>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This customer bought lots of children's gifts and some holiday items. \n\nLook at the descriptions of the recommended products to see if they are in the same categories.\n\n<div class=\"alert alert-block alert-info\">Note: The ALS algorithm uses some randomness, so the recommendations you get might be different from these.</div>", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "source": "stockItems = sqlContext.sql(\"select distinct stockCode, description from retailPurchases\")\nstockItems.registerTempTable(\"stockItems\")\n\nquery = \"\"\"\nselect \n    predictions.*,\n    stockItems.description\nfrom\n    predictions\ninner join stockItems on\n    predictions.stockCode = stockItems.stockCode\norder by predictions.prediction desc\nlimit 10\n\"\"\"\nsqlContext.sql(query).toPandas()", 
            "outputs": [
                {
                    "execution_count": 18, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "   stockCode  custId  prediction                          description\n0      22326   15544    0.529536  ROUND SNACK BOXES SET OF4 WOODLAND \n1      21242   15544    0.524287                 RED RETROSPOT PLATE \n2      21559   15544    0.501251    STRAWBERRY LUNCH BOX WITH CUTLERY\n3      22090   15544    0.490788              PAPER BUNTING RETROSPOT\n4      22367   15544    0.482872      CHILDRENS APRON SPACEBOY DESIGN\n5      21987   15544    0.476816           PACK OF 6 SKULL PAPER CUPS\n6      21122   15544    0.467991   SET/10 PINK POLKADOT PARTY CANDLES\n7      22417   15544    0.461298       PACK OF 60 SPACEBOY CAKE CASES\n8      21124   15544    0.456542   SET/10 BLUE POLKADOT PARTY CANDLES\n9      21156   15544    0.455864            RETROSPOT CHILDRENS APRON", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stockCode</th>\n      <th>custId</th>\n      <th>prediction</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22326</td>\n      <td>15544</td>\n      <td>0.529536</td>\n      <td>ROUND SNACK BOXES SET OF4 WOODLAND</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21242</td>\n      <td>15544</td>\n      <td>0.524287</td>\n      <td>RED RETROSPOT PLATE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21559</td>\n      <td>15544</td>\n      <td>0.501251</td>\n      <td>STRAWBERRY LUNCH BOX WITH CUTLERY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22090</td>\n      <td>15544</td>\n      <td>0.490788</td>\n      <td>PAPER BUNTING RETROSPOT</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22367</td>\n      <td>15544</td>\n      <td>0.482872</td>\n      <td>CHILDRENS APRON SPACEBOY DESIGN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>21987</td>\n      <td>15544</td>\n      <td>0.476816</td>\n      <td>PACK OF 6 SKULL PAPER CUPS</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>21122</td>\n      <td>15544</td>\n      <td>0.467991</td>\n      <td>SET/10 PINK POLKADOT PARTY CANDLES</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>22417</td>\n      <td>15544</td>\n      <td>0.461298</td>\n      <td>PACK OF 60 SPACEBOY CAKE CASES</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>21124</td>\n      <td>15544</td>\n      <td>0.456542</td>\n      <td>SET/10 BLUE POLKADOT PARTY CANDLES</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>21156</td>\n      <td>15544</td>\n      <td>0.455864</td>\n      <td>RETROSPOT CHILDRENS APRON</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "metadata": {}
                }
            ], 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "source": "The recommended products look pretty similar to the purchased products, and, in some cases, are actually the same. Your model works!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"summary\"></a>\n## 7. Summary and next steps\nYou created a predictive model that makes product recommendations for customers and verified that it works.\n\nDig deeper:\n - [Collaborative Filtering](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html)\n - [Spark Machine Learning Library (MLlib) Guide](http://spark.apache.org/docs/latest/ml-guide.html)\n - [Spark Python API Docs](http://spark.apache.org/docs/latest/api/python/index.html)\n\n\n### Authors\n**Carlo Appugliese** is a Spark and Hadoop evangelist at IBM.<br>\n**Braden Callahan** is a Big Data Technical Specialist for IBM.<br>\n**Ross Lewis** is a Big Data Technical Sales Specialist for IBM.<br>\n**Mokhtar Kandil** is a World Wide Big Data Technical Specialist for IBM.\n\n\n## Data citation\nDaqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208, 2012 (Published online before print: 27 August 2012. doi: 10.1057/dbm.2012.17).\n\nChen, D. (2012). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<hr>\nCopyright &copy; IBM Corp. 2017, 2018. This notebook and its source code are released under the terms of the MIT License.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }
}